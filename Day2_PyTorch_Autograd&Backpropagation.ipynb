{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9123386e",
   "metadata": {},
   "source": [
    "üìö 1. What is Autograd?\n",
    "\n",
    "autograd is a PyTorch feature to automatically calculate the derivative (gradient). Very important when training neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf99fb7",
   "metadata": {},
   "source": [
    "üß™ 2. Autograd Basic Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a716d63d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradien of y with respect to x:  tensor(7.)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create a tensor with gradient traking\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "# Define a simple function : Y = x^2 + 3x + 1\n",
    "y = x**2 + 3*x + 1\n",
    "\n",
    "# Backpropagate\n",
    "y.backward()\n",
    "\n",
    "# Print gradient dy/dx\n",
    "print(\"Gradien of y with respect to x: \", x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8036450d",
   "metadata": {},
   "source": [
    "üí° Penjelasan:\n",
    "\n",
    "1. (Autogradrequires_grad=True) makes PyTorch keep track of all operations on the tensor.\n",
    "2. (.backward() )calculates the derivative.\n",
    "3. The gradient is stored in (x.grad)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fa5abf",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è 3. Non-Tracking Mode (no_grad)\n",
    "\n",
    "Sometimes we don't need gradients (for example during model evaluation). We can disable autograd using (torch.no_grad():)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28fb33c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y (no grad): tensor(4.)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    y = x * 2\n",
    "    print(\"y (no grad):\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b365158f",
   "metadata": {},
   "source": [
    "üßÆ 4. More Complex Gradient Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "edcdca05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradien x[0]: tensor(4.)\n",
      "Gradien x[1]: tensor(6.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([2.0, 3.0], requires_grad=True)\n",
    "\n",
    "# Function : z = x1^2 = x2^2\n",
    "z = x[0]**2 + x[1]**2\n",
    "z.backward()\n",
    "\n",
    "print(\"Gradien x[0]:\", x.grad[0])  # should be 2*x[0]\n",
    "print(\"Gradien x[1]:\", x.grad[1])  # should be 3*x[1]^2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4c2d36",
   "metadata": {},
   "source": [
    "‚úÖ Exercise Day 2:\n",
    "\n",
    "1. Create a function y=3x3+2x2+xy=3x3+2x2+x, calculate the derivative with respect to x = 4.0.\n",
    "2. Use torch.no_grad() to calculate the function result without gradient tracking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f00688",
   "metadata": {},
   "source": [
    "‚úÖ 1. Autograd for Function y=3x^3 + 2x^2 + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27ed21c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x= 4.0\n",
      "y= 228.0\n",
      "dx/dy= 161.0\n"
     ]
    }
   ],
   "source": [
    "# Define x as a tensor with grad enabled\n",
    "x = torch.tensor(4.0, requires_grad=True)\n",
    "\n",
    "# Define the function: y = 3x^2 + 2x^2 + x\n",
    "y = 3 * x**3 + 2 * x**2 + x\n",
    "\n",
    "# compute gradient\n",
    "y.backward()\n",
    "\n",
    "# print the gradient dy/dx\n",
    "print(\"x=\", x.item())\n",
    "print(\"y=\", y.item())\n",
    "print(\"dx/dy=\", x.grad.item())"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "9051e84b",
   "metadata": {},
   "source": [
    "üí° Turunan fungsi secara manual:\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "Coba cek apakah hasilnya sesuai ya."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1482e677",
   "metadata": {},
   "source": [
    "‚úÖ 2. Autograd for FunctionCalculate Function Value Without Gradient Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e3c5b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y (No grad)= 228.0\n"
     ]
    }
   ],
   "source": [
    "# Disable gradient traking\n",
    "with torch.no_grad():\n",
    "    x_val = torch.tensor(4.0)\n",
    "    y_val = 3 * x_val**3 + 2 * x_val**2 + x_val\n",
    "    print(\"y (No grad)=\", y_val.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
